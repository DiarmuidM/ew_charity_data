
Data collection:
- import.py and bcp.py scripts are the work of https://github.com/ncvo/charity-commission-extract
- amend ew_download and fimport to process SIRs data
- amend ew_download to process other months of the Charity Register
- trustee_info.py:
	o write to json and csv [DONE]
	o implement an error log i.e. regno, link, attempts, success, number of regnos processed and write to an external file [DONE-ISH]
	o write a function [DONE - see Tom's syntax]
	o synthesise the two files (leave Tom's untouched)
	o deal with the row index number (starts at zero for charities with trustee information)
	o capture subsidiary information?
	o grab reason for removal
	o order of writing from dictionary to csv [DONE]
	o track the amount of charity numbers the script has processed [DONE]
	o start the script at a particular row in the dataframe [DONE]
	o Companies House API search for directors; just export ['items'] key values
	o deal with financial year end missing data for charities overdue
	o adjust script to scrape NI and Rep. Ire charity data.
	o run scrape on Raspberry Pi every month



Data analysis and management:
- analysis of trustee network:
- gender identification (Miss, Mr + Lecy/Rutherford script)
- network analysis (http://kateto.net/networks-r-igraph)



- Richer descriptives
- New variables for mlogit (e.g. trustees)
- Clean up Register download to create dataset
- Model diagnostics
- I don't know if you thought of it but you can of course transpose the matrix prior to svmat, 
so you have 1 column with 35 rows, etc

I tend to use results this way, and I would typically stack the results from different models below, 
e.g. rows 1-35 for model 1, 36-70 for model 2, etc
Set up that way, you can add extra variables to indicate model and parameter type, 
then use them as 'by', 'over', and 'if' conditions in graphs, e.g. "graph bar (mean) regres if var_id=1, over(model_id) " , etc


